{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086aec1f",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989c5aa",
   "metadata": {},
   "source": [
    "## üìä Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac77aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one CSV file for initial analysis\n",
    "sample_file = '../data/raw_csv/02-14-2018.csv'\n",
    "print(f\"Loading sample data from: {sample_file}\")\n",
    "\n",
    "# Read sample data\n",
    "df_sample = pd.read_csv(sample_file)\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "print(f\"Memory usage: {df_sample.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Number of records: {len(df_sample):,}\")\n",
    "print(f\"Number of features: {len(df_sample.columns):,}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_sample.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"=== DATA TYPES & MISSING VALUES ===\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df_sample.columns,\n",
    "    'Data_Type': df_sample.dtypes,\n",
    "    'Non_Null_Count': df_sample.count(),\n",
    "    'Null_Count': df_sample.isnull().sum(),\n",
    "    'Null_Percentage': (df_sample.isnull().sum() / len(df_sample) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_cols = info_df[info_df['Null_Count'] > 0].sort_values('Null_Percentage', ascending=False)\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    display(missing_cols)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "\n",
    "print(f\"\\nData type distribution:\")\n",
    "print(info_df['Data_Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581b660",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Label Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f19b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Label column (target variable)\n",
    "print(\"=== LABEL DISTRIBUTION ===\")\n",
    "label_counts = df_sample['Label'].value_counts()\n",
    "label_percentages = df_sample['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "label_summary = pd.DataFrame({\n",
    "    'Count': label_counts,\n",
    "    'Percentage': label_percentages.round(2)\n",
    "})\n",
    "\n",
    "print(label_summary)\n",
    "\n",
    "# Check for class imbalance\n",
    "benign_ratio = label_percentages.get('Benign', 0)\n",
    "attack_ratio = 100 - benign_ratio\n",
    "\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "print(f\"Benign Traffic: {benign_ratio:.2f}%\")\n",
    "print(f\"Attack Traffic: {attack_ratio:.2f}%\")\n",
    "\n",
    "if benign_ratio > 90 or benign_ratio < 10:\n",
    "    print(\"‚ö†Ô∏è Highly imbalanced dataset detected!\")\n",
    "elif benign_ratio > 80 or benign_ratio < 20:\n",
    "    print(\"‚ö° Moderately imbalanced dataset\")\n",
    "else:\n",
    "    print(\"‚úÖ Relatively balanced dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart\n",
    "axes[0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Label Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "sns.countplot(data=df_sample, x='Label', ax=axes[1])\n",
    "axes[1].set_title('Label Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive plotly chart\n",
    "fig = px.bar(x=label_counts.index, y=label_counts.values, \n",
    "             title='Interactive Label Distribution',\n",
    "             labels={'x': 'Attack Type', 'y': 'Count'},\n",
    "             color=label_counts.values,\n",
    "             color_continuous_scale='viridis')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b2240",
   "metadata": {},
   "source": [
    "## üìà Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b81a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numerical_cols = df_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Label' in numerical_cols:\n",
    "    numerical_cols.remove('Label')\n",
    "\n",
    "print(f\"Found {len(numerical_cols)} numerical features\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== NUMERICAL FEATURES STATISTICS ===\")\n",
    "stats = df_sample[numerical_cols].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"=== INFINITE VALUES CHECK ===\")\n",
    "inf_counts = {}\n",
    "for col in numerical_cols:\n",
    "    inf_count = np.isinf(df_sample[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    inf_df = pd.DataFrame(list(inf_counts.items()), columns=['Column', 'Infinite_Count'])\n",
    "    inf_df['Percentage'] = (inf_df['Infinite_Count'] / len(df_sample) * 100).round(2)\n",
    "    print(\"Columns with infinite values:\")\n",
    "    display(inf_df.sort_values('Infinite_Count', ascending=False))\n",
    "else:\n",
    "    print(\"‚úÖ No infinite values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc765ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "key_features = ['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', \n",
    "               'Total Length of Fwd Packets', 'Total Length of Bwd Packets']\n",
    "\n",
    "# Find actual column names (they might have slight differences)\n",
    "actual_features = []\n",
    "for feature in key_features:\n",
    "    matches = [col for col in df_sample.columns if feature.lower().replace(' ', '') in col.lower().replace(' ', '').replace('_', '')]\n",
    "    if matches:\n",
    "        actual_features.append(matches[0])\n",
    "\n",
    "print(f\"Analyzing distributions for: {actual_features[:5]}\")\n",
    "\n",
    "# Plot distributions\n",
    "if actual_features:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(actual_features[:6]):\n",
    "        if i < len(axes):\n",
    "            # Remove infinite and very large values for visualization\n",
    "            clean_data = df_sample[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(clean_data) > 0:\n",
    "                # Use log scale for better visualization\n",
    "                log_data = np.log1p(clean_data)\n",
    "                axes[i].hist(log_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(f'Log Distribution: {feature}', fontweight='bold')\n",
    "                axes[i].set_xlabel('Log(value + 1)')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].set_title(f'No Data: {feature}')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(len(actual_features), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb1fc5",
   "metadata": {},
   "source": [
    "## üîó Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of features for correlation analysis (to avoid memory issues)\n",
    "sample_features = numerical_cols[:20]  # First 20 numerical features\n",
    "\n",
    "print(f\"Calculating correlations for {len(sample_features)} features...\")\n",
    "\n",
    "# Calculate correlation matrix (handle infinite values)\n",
    "df_corr = df_sample[sample_features].replace([np.inf, -np.inf], np.nan)\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Feature Correlation Heatmap (Lower Triangle)', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if not np.isnan(corr_value) and abs(corr_value) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': round(corr_value, 3)\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.8):\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    display(high_corr_df.head(10))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No highly correlated features found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8252e",
   "metadata": {},
   "source": [
    "## üéØ Attack vs Benign Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target\n",
    "df_sample['is_attack'] = (df_sample['Label'] != 'Benign').astype(int)\n",
    "\n",
    "# Compare key features between benign and attack traffic\n",
    "comparison_features = actual_features[:6] if actual_features else numerical_cols[:6]\n",
    "\n",
    "print(\"=== BENIGN vs ATTACK COMPARISON ===\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(comparison_features):\n",
    "    if i < len(axes) and feature in df_sample.columns:\n",
    "        # Clean data\n",
    "        clean_data = df_sample[[feature, 'is_attack']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(clean_data) > 0:\n",
    "            # Box plot comparison\n",
    "            sns.boxplot(data=clean_data, x='is_attack', y=feature, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature}\\nBenign (0) vs Attack (1)', fontweight='bold')\n",
    "            axes[i].set_yscale('log')  # Log scale for better visualization\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=axes[i].transAxes)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(len(comparison_features), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "print(\"\\n=== STATISTICAL COMPARISON ===\")\n",
    "comparison_stats = []\n",
    "\n",
    "for feature in comparison_features:\n",
    "    if feature in df_sample.columns:\n",
    "        clean_data = df_sample[[feature, 'is_attack']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(clean_data) > 0:\n",
    "            benign_data = clean_data[clean_data['is_attack'] == 0][feature]\n",
    "            attack_data = clean_data[clean_data['is_attack'] == 1][feature]\n",
    "            \n",
    "            if len(benign_data) > 0 and len(attack_data) > 0:\n",
    "                comparison_stats.append({\n",
    "                    'Feature': feature,\n",
    "                    'Benign_Mean': benign_data.mean(),\n",
    "                    'Attack_Mean': attack_data.mean(),\n",
    "                    'Benign_Std': benign_data.std(),\n",
    "                    'Attack_Std': attack_data.std(),\n",
    "                    'Mean_Ratio': attack_data.mean() / benign_data.mean() if benign_data.mean() != 0 else np.inf\n",
    "                })\n",
    "\n",
    "if comparison_stats:\n",
    "    comp_df = pd.DataFrame(comparison_stats)\n",
    "    comp_df = comp_df.round(4)\n",
    "    display(comp_df)\n",
    "else:\n",
    "    print(\"No valid data for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b5ca8",
   "metadata": {},
   "source": [
    "## üìä Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624751cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "\n",
    "# Calculate quality metrics\n",
    "total_rows = len(df_sample)\n",
    "total_cols = len(df_sample.columns)\n",
    "\n",
    "# Missing values\n",
    "total_missing = df_sample.isnull().sum().sum()\n",
    "missing_percentage = (total_missing / (total_rows * total_cols)) * 100\n",
    "\n",
    "# Duplicate rows\n",
    "duplicate_rows = df_sample.duplicated().sum()\n",
    "duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "\n",
    "# Infinite values in numerical columns\n",
    "total_inf = 0\n",
    "for col in numerical_cols:\n",
    "    total_inf += np.isinf(df_sample[col]).sum()\n",
    "\n",
    "# Constant columns (zero variance)\n",
    "constant_cols = []\n",
    "for col in numerical_cols:\n",
    "    if df_sample[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "# Quality summary\n",
    "quality_report = {\n",
    "    'Total Records': f\"{total_rows:,}\",\n",
    "    'Total Features': f\"{total_cols:,}\",\n",
    "    'Missing Values': f\"{total_missing:,} ({missing_percentage:.2f}%)\",\n",
    "    'Duplicate Rows': f\"{duplicate_rows:,} ({duplicate_percentage:.2f}%)\",\n",
    "    'Infinite Values': f\"{total_inf:,}\",\n",
    "    'Constant Features': f\"{len(constant_cols)}\",\n",
    "    'Numerical Features': f\"{len(numerical_cols)}\",\n",
    "    'Categorical Features': f\"{len(df_sample.select_dtypes(include=['object']).columns)}\"\n",
    "}\n",
    "\n",
    "for key, value in quality_report.items():\n",
    "    print(f\"{key:<20}: {value}\")\n",
    "\n",
    "# Data quality score\n",
    "quality_score = 100\n",
    "if missing_percentage > 5:\n",
    "    quality_score -= min(missing_percentage * 2, 30)\n",
    "if duplicate_percentage > 1:\n",
    "    quality_score -= min(duplicate_percentage, 20)\n",
    "if total_inf > 0:\n",
    "    quality_score -= 10\n",
    "if len(constant_cols) > 0:\n",
    "    quality_score -= len(constant_cols)\n",
    "\n",
    "quality_score = max(quality_score, 0)\n",
    "\n",
    "print(f\"\\nüìä Overall Data Quality Score: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    print(\"‚úÖ Excellent data quality!\")\n",
    "elif quality_score >= 70:\n",
    "    print(\"‚ú® Good data quality with minor issues\")\n",
    "elif quality_score >= 50:\n",
    "    print(\"‚ö†Ô∏è Moderate data quality - needs attention\")\n",
    "else:\n",
    "    print(\"‚ùå Poor data quality - significant preprocessing required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca429283",
   "metadata": {},
   "source": [
    "## üìã Preprocessing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ce0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PREPROCESSING RECOMMENDATIONS ===\")\n",
    "recommendations = []\n",
    "\n",
    "# Missing values\n",
    "if total_missing > 0:\n",
    "    recommendations.append(\"üîß Handle missing values using imputation or removal\")\n",
    "\n",
    "# Infinite values\n",
    "if total_inf > 0:\n",
    "    recommendations.append(\"üîß Replace infinite values with NaN or large finite numbers\")\n",
    "\n",
    "# Duplicates\n",
    "if duplicate_rows > 0:\n",
    "    recommendations.append(\"üîß Remove duplicate rows to avoid data leakage\")\n",
    "\n",
    "# Constant features\n",
    "if constant_cols:\n",
    "    recommendations.append(f\"üîß Remove {len(constant_cols)} constant features (zero variance)\")\n",
    "\n",
    "# High correlations\n",
    "if high_corr_pairs:\n",
    "    recommendations.append(f\"üîß Consider removing {len(high_corr_pairs)} highly correlated feature pairs\")\n",
    "\n",
    "# Class imbalance\n",
    "if benign_ratio > 90 or benign_ratio < 10:\n",
    "    recommendations.append(\"üîß Address severe class imbalance using SMOTE, undersampling, or weighted models\")\n",
    "elif benign_ratio > 80 or benign_ratio < 20:\n",
    "    recommendations.append(\"üîß Consider techniques for moderate class imbalance\")\n",
    "\n",
    "# Feature scaling\n",
    "recommendations.append(\"üîß Apply feature scaling (StandardScaler or MinMaxScaler)\")\n",
    "\n",
    "# Feature selection\n",
    "recommendations.append(\"üîß Perform feature selection to reduce dimensionality\")\n",
    "\n",
    "# Time series\n",
    "recommendations.append(\"üîß Create time-based windows for time series analysis\")\n",
    "\n",
    "# Display recommendations\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nüìà Ready to proceed with Big Data preprocessing using Apache Spark!\")\n",
    "print(f\"üíæ Next step: Run the Spark preprocessing pipeline to handle the full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a1038",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Run Spark Preprocessing**: Execute `spark/merge_clean.py` to process all CSV files\n",
    "2. **Feature Engineering**: Run `spark/feature_engineering.py` for advanced features\n",
    "3. **Time Series Analysis**: Create temporal windows and sequence features\n",
    "4. **Model Building**: Train Random Forest, XGBoost, and LSTM models\n",
    "5. **Real-time IDS**: Implement streaming detection pipeline\n",
    "\n",
    "This EDA provides the foundation for understanding the CSE-CIC-IDS2018 dataset structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=== PCA ANALYSIS ===\")\n",
    "\n",
    "# Sample data for PCA (use subset for performance)\n",
    "sample_size = min(10000, len(df_sample))\n",
    "df_pca = df_sample.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Select numerical features and clean\n",
    "pca_features = numerical_cols[:20]  # Use first 20 features\n",
    "X_pca = df_pca[pca_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_pca_transformed = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original dimensions: {X_scaled.shape}\")\n",
    "print(f\"Reduced dimensions: {X_pca_transformed.shape}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_[:5], 1):\n",
    "    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%)\")\n",
    "print(f\"  Cumulative (first 5): {sum(pca.explained_variance_ratio_[:5]):.4f} ({sum(pca.explained_variance_ratio_[:5])*100:.2f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scree plot\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, 11), pca.explained_variance_ratio_, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax1.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_), 'r-o', linewidth=2, label='Cumulative')\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('PCA - Scree Plot', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2D PCA scatter\n",
    "ax2 = axes[1]\n",
    "y_pca = df_pca['is_attack'].values\n",
    "scatter = ax2.scatter(X_pca_transformed[:, 0], X_pca_transformed[:, 1], \n",
    "                     c=y_pca, cmap='coolwarm', alpha=0.6, s=20, edgecolors='k', linewidth=0.5)\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "ax2.set_title('PCA - 2D Projection (Benign vs Attack)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax2, label='Class (0=Benign, 1=Attack)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ PCA analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"=== OUTLIER DETECTION ===\")\n",
    "\n",
    "# Use subset of features for outlier detection\n",
    "outlier_features = comparison_features[:4] if comparison_features else numerical_cols[:4]\n",
    "X_outlier = df_sample[outlier_features].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_predictions = iso_forest.fit_predict(X_outlier)\n",
    "\n",
    "# Count outliers\n",
    "outliers_count = (outlier_predictions == -1).sum()\n",
    "outliers_percentage = (outliers_count / len(df_sample)) * 100\n",
    "\n",
    "print(f\"Total samples: {len(df_sample):,}\")\n",
    "print(f\"Detected outliers: {outliers_count:,} ({outliers_percentage:.2f}%)\")\n",
    "\n",
    "# Outliers by class\n",
    "df_sample['is_outlier'] = (outlier_predictions == -1).astype(int)\n",
    "\n",
    "outlier_by_class = df_sample.groupby(['is_attack', 'is_outlier']).size().unstack(fill_value=0)\n",
    "print(f\"\\nOutliers by class:\")\n",
    "print(outlier_by_class)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Outlier distribution\n",
    "ax1 = axes[0]\n",
    "outlier_counts = df_sample.groupby(['is_attack', 'is_outlier']).size().unstack(fill_value=0)\n",
    "outlier_counts.plot(kind='bar', ax=ax1, color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Outliers by Class', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class (0=Benign, 1=Attack)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.legend(['Normal', 'Outlier'])\n",
    "ax1.set_xticklabels(['Benign', 'Attack'], rotation=0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PCA with outliers highlighted\n",
    "ax2 = axes[1]\n",
    "normal_mask = outlier_predictions == 1\n",
    "outlier_mask = outlier_predictions == -1\n",
    "\n",
    "ax2.scatter(X_pca_transformed[normal_mask, 0], X_pca_transformed[normal_mask, 1],\n",
    "           c='blue', alpha=0.3, s=10, label='Normal', edgecolors='none')\n",
    "ax2.scatter(X_pca_transformed[outlier_mask, 0], X_pca_transformed[outlier_mask, 1],\n",
    "           c='red', alpha=0.7, s=30, label='Outlier', marker='x', linewidths=2)\n",
    "ax2.set_xlabel('PC1', fontsize=12)\n",
    "ax2.set_ylabel('PC2', fontsize=12)\n",
    "ax2.set_title('Outliers in PCA Space', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Outlier detection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TEMPORAL ATTACK PATTERNS ===\")\n",
    "\n",
    "# Create synthetic timestamps (since we don't have real timestamps in sample)\n",
    "df_sample['timestamp'] = pd.date_range(start='2018-02-14 00:00:00', periods=len(df_sample), freq='1s')\n",
    "df_sample['hour'] = df_sample['timestamp'].dt.hour\n",
    "df_sample['day_of_week'] = df_sample['timestamp'].dt.dayofweek\n",
    "df_sample['date'] = df_sample['timestamp'].dt.date\n",
    "\n",
    "# Attack counts by hour\n",
    "hourly_attacks = df_sample[df_sample['is_attack'] == 1].groupby('hour').size()\n",
    "hourly_total = df_sample.groupby('hour').size()\n",
    "hourly_attack_rate = (hourly_attacks / hourly_total * 100).fillna(0)\n",
    "\n",
    "print(f\"\\nAttacks by hour of day:\")\n",
    "print(f\"Peak attack hour: {hourly_attacks.idxmax()} with {hourly_attacks.max()} attacks\")\n",
    "print(f\"Lowest attack hour: {hourly_attacks.idxmin()} with {hourly_attacks.min()} attacks\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Hourly attack counts\n",
    "ax1 = axes[0, 0]\n",
    "hourly_attacks.plot(kind='bar', ax=ax1, color='crimson', alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Attack Count by Hour of Day', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Hour')\n",
    "ax1.set_ylabel('Attack Count')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Attack rate by hour\n",
    "ax2 = axes[0, 1]\n",
    "hourly_attack_rate.plot(kind='line', ax=ax2, color='darkred', linewidth=3, marker='o')\n",
    "ax2.fill_between(range(24), hourly_attack_rate.values, alpha=0.3, color='red')\n",
    "ax2.set_title('Attack Rate by Hour of Day', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Hour')\n",
    "ax2.set_ylabel('Attack Rate (%)')\n",
    "ax2.set_xticks(range(24))\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Day of week analysis\n",
    "ax3 = axes[1, 0]\n",
    "day_attacks = df_sample[df_sample['is_attack'] == 1].groupby('day_of_week').size()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_attacks.index = [day_names[i] for i in day_attacks.index]\n",
    "day_attacks.plot(kind='bar', ax=ax3, color='orange', alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('Attack Count by Day of Week', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Day')\n",
    "ax3.set_ylabel('Attack Count')\n",
    "ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Attack type distribution over time\n",
    "ax4 = axes[1, 1]\n",
    "# Get top attack types\n",
    "top_attacks = df_sample[df_sample['Label'] != 'Benign']['Label'].value_counts().head(5)\n",
    "attack_timeline = df_sample[df_sample['Label'].isin(top_attacks.index)].groupby(['hour', 'Label']).size().unstack(fill_value=0)\n",
    "\n",
    "if not attack_timeline.empty:\n",
    "    attack_timeline.plot(kind='area', stacked=True, ax=ax4, alpha=0.7)\n",
    "    ax4.set_title('Attack Types Distribution by Hour', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Hour')\n",
    "    ax4.set_ylabel('Attack Count')\n",
    "    ax4.legend(title='Attack Type', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No attack type data available', ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Temporal pattern analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec59a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE DISTRIBUTIONS BY ATTACK TYPE ===\")\n",
    "\n",
    "# Get top 5 attack types + Benign\n",
    "top_labels = df_sample['Label'].value_counts().head(6).index.tolist()\n",
    "df_attack_analysis = df_sample[df_sample['Label'].isin(top_labels)]\n",
    "\n",
    "# Select features for analysis\n",
    "analysis_features = comparison_features[:3] if comparison_features else numerical_cols[:3]\n",
    "\n",
    "print(f\"\\nAnalyzing {len(analysis_features)} features across {len(top_labels)} classes\")\n",
    "print(f\"Classes: {', '.join(top_labels)}\")\n",
    "\n",
    "fig, axes = plt.subplots(len(analysis_features), 1, figsize=(16, 5*len(analysis_features)))\n",
    "\n",
    "if len(analysis_features) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, feature in enumerate(analysis_features):\n",
    "    if feature in df_attack_analysis.columns:\n",
    "        # Clean data\n",
    "        plot_data = df_attack_analysis[[feature, 'Label']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(plot_data) > 0:\n",
    "            # Violin plot\n",
    "            sns.violinplot(data=plot_data, x='Label', y=feature, ax=axes[idx], palette='Set2')\n",
    "            axes[idx].set_title(f'Distribution of {feature} by Attack Type', fontsize=14, fontweight='bold')\n",
    "            axes[idx].set_xlabel('Attack Type')\n",
    "            axes[idx].set_ylabel(feature)\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "            axes[idx].set_yscale('log')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, f'No valid data for {feature}', \n",
    "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature distribution analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b9aa6",
   "metadata": {},
   "source": [
    "## üìä Advanced Feature Analysis - Distributions by Attack Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950d09e",
   "metadata": {},
   "source": [
    "## ‚è∞ Temporal Patterns & Attack Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da784e7",
   "metadata": {},
   "source": [
    "## üîç Outlier Detection & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ec2b4",
   "metadata": {},
   "source": [
    "## üé® Advanced Visualizations - PCA & Dimensionality Reduction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

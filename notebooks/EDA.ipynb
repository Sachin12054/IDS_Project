{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086aec1f",
   "metadata": {},
   "source": [
    "## ðŸ“š Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"ðŸ“¦ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989c5aa",
   "metadata": {},
   "source": [
    "## ðŸ“Š Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac77aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one CSV file for initial analysis\n",
    "sample_file = '../data/raw_csv/02-14-2018.csv'\n",
    "print(f\"Loading sample data from: {sample_file}\")\n",
    "\n",
    "# Read sample data\n",
    "df_sample = pd.read_csv(sample_file)\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"Shape: {df_sample.shape}\")\n",
    "print(f\"Memory usage: {df_sample.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Number of records: {len(df_sample):,}\")\n",
    "print(f\"Number of features: {len(df_sample.columns):,}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_sample.columns):\n",
    "    print(f\"{i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "print(\"=== DATA TYPES & MISSING VALUES ===\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df_sample.columns,\n",
    "    'Data_Type': df_sample.dtypes,\n",
    "    'Non_Null_Count': df_sample.count(),\n",
    "    'Null_Count': df_sample.isnull().sum(),\n",
    "    'Null_Percentage': (df_sample.isnull().sum() / len(df_sample) * 100).round(2)\n",
    "})\n",
    "\n",
    "# Show columns with missing values\n",
    "missing_cols = info_df[info_df['Null_Count'] > 0].sort_values('Null_Percentage', ascending=False)\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    display(missing_cols)\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found!\")\n",
    "\n",
    "print(f\"\\nData type distribution:\")\n",
    "print(info_df['Data_Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581b660",
   "metadata": {},
   "source": [
    "## ðŸ·ï¸ Label Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f19b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Label column (target variable)\n",
    "print(\"=== LABEL DISTRIBUTION ===\")\n",
    "label_counts = df_sample['Label'].value_counts()\n",
    "label_percentages = df_sample['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "label_summary = pd.DataFrame({\n",
    "    'Count': label_counts,\n",
    "    'Percentage': label_percentages.round(2)\n",
    "})\n",
    "\n",
    "print(label_summary)\n",
    "\n",
    "# Check for class imbalance\n",
    "benign_ratio = label_percentages.get('Benign', 0)\n",
    "attack_ratio = 100 - benign_ratio\n",
    "\n",
    "print(f\"\\nðŸ“Š Class Distribution:\")\n",
    "print(f\"Benign Traffic: {benign_ratio:.2f}%\")\n",
    "print(f\"Attack Traffic: {attack_ratio:.2f}%\")\n",
    "\n",
    "if benign_ratio > 90 or benign_ratio < 10:\n",
    "    print(\"âš ï¸ Highly imbalanced dataset detected!\")\n",
    "elif benign_ratio > 80 or benign_ratio < 20:\n",
    "    print(\"âš¡ Moderately imbalanced dataset\")\n",
    "else:\n",
    "    print(\"âœ… Relatively balanced dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie chart\n",
    "axes[0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Label Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "sns.countplot(data=df_sample, x='Label', ax=axes[1])\n",
    "axes[1].set_title('Label Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive plotly chart\n",
    "fig = px.bar(x=label_counts.index, y=label_counts.values, \n",
    "             title='Interactive Label Distribution',\n",
    "             labels={'x': 'Attack Type', 'y': 'Count'},\n",
    "             color=label_counts.values,\n",
    "             color_continuous_scale='viridis')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1b2240",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b81a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numerical_cols = df_sample.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Label' in numerical_cols:\n",
    "    numerical_cols.remove('Label')\n",
    "\n",
    "print(f\"Found {len(numerical_cols)} numerical features\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n=== NUMERICAL FEATURES STATISTICS ===\")\n",
    "stats = df_sample[numerical_cols].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values\n",
    "print(\"=== INFINITE VALUES CHECK ===\")\n",
    "inf_counts = {}\n",
    "for col in numerical_cols:\n",
    "    inf_count = np.isinf(df_sample[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        inf_counts[col] = inf_count\n",
    "\n",
    "if inf_counts:\n",
    "    inf_df = pd.DataFrame(list(inf_counts.items()), columns=['Column', 'Infinite_Count'])\n",
    "    inf_df['Percentage'] = (inf_df['Infinite_Count'] / len(df_sample) * 100).round(2)\n",
    "    print(\"Columns with infinite values:\")\n",
    "    display(inf_df.sort_values('Infinite_Count', ascending=False))\n",
    "else:\n",
    "    print(\"âœ… No infinite values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc765ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "key_features = ['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', \n",
    "               'Total Length of Fwd Packets', 'Total Length of Bwd Packets']\n",
    "\n",
    "# Find actual column names (they might have slight differences)\n",
    "actual_features = []\n",
    "for feature in key_features:\n",
    "    matches = [col for col in df_sample.columns if feature.lower().replace(' ', '') in col.lower().replace(' ', '').replace('_', '')]\n",
    "    if matches:\n",
    "        actual_features.append(matches[0])\n",
    "\n",
    "print(f\"Analyzing distributions for: {actual_features[:5]}\")\n",
    "\n",
    "# Plot distributions\n",
    "if actual_features:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(actual_features[:6]):\n",
    "        if i < len(axes):\n",
    "            # Remove infinite and very large values for visualization\n",
    "            clean_data = df_sample[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(clean_data) > 0:\n",
    "                # Use log scale for better visualization\n",
    "                log_data = np.log1p(clean_data)\n",
    "                axes[i].hist(log_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(f'Log Distribution: {feature}', fontweight='bold')\n",
    "                axes[i].set_xlabel('Log(value + 1)')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=axes[i].transAxes)\n",
    "                axes[i].set_title(f'No Data: {feature}')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for j in range(len(actual_features), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb1fc5",
   "metadata": {},
   "source": [
    "## ðŸ”— Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of features for correlation analysis (to avoid memory issues)\n",
    "sample_features = numerical_cols[:20]  # First 20 numerical features\n",
    "\n",
    "print(f\"Calculating correlations for {len(sample_features)} features...\")\n",
    "\n",
    "# Calculate correlation matrix (handle infinite values)\n",
    "df_corr = df_sample[sample_features].replace([np.inf, -np.inf], np.nan)\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Feature Correlation Heatmap (Lower Triangle)', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if not np.isnan(corr_value) and abs(corr_value) > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': round(corr_value, 3)\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nâš ï¸ Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.8):\")\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', key=abs, ascending=False)\n",
    "    display(high_corr_df.head(10))\n",
    "else:\n",
    "    print(\"\\nâœ… No highly correlated features found (|r| > 0.8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8252e",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Attack vs Benign Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target\n",
    "df_sample['is_attack'] = (df_sample['Label'] != 'Benign').astype(int)\n",
    "\n",
    "# Compare key features between benign and attack traffic\n",
    "comparison_features = actual_features[:6] if actual_features else numerical_cols[:6]\n",
    "\n",
    "print(\"=== BENIGN vs ATTACK COMPARISON ===\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(comparison_features):\n",
    "    if i < len(axes) and feature in df_sample.columns:\n",
    "        # Clean data\n",
    "        clean_data = df_sample[[feature, 'is_attack']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(clean_data) > 0:\n",
    "            # Box plot comparison\n",
    "            sns.boxplot(data=clean_data, x='is_attack', y=feature, ax=axes[i])\n",
    "            axes[i].set_title(f'{feature}\\nBenign (0) vs Attack (1)', fontweight='bold')\n",
    "            axes[i].set_yscale('log')  # Log scale for better visualization\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=axes[i].transAxes)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(len(comparison_features), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "print(\"\\n=== STATISTICAL COMPARISON ===\")\n",
    "comparison_stats = []\n",
    "\n",
    "for feature in comparison_features:\n",
    "    if feature in df_sample.columns:\n",
    "        clean_data = df_sample[[feature, 'is_attack']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        \n",
    "        if len(clean_data) > 0:\n",
    "            benign_data = clean_data[clean_data['is_attack'] == 0][feature]\n",
    "            attack_data = clean_data[clean_data['is_attack'] == 1][feature]\n",
    "            \n",
    "            if len(benign_data) > 0 and len(attack_data) > 0:\n",
    "                comparison_stats.append({\n",
    "                    'Feature': feature,\n",
    "                    'Benign_Mean': benign_data.mean(),\n",
    "                    'Attack_Mean': attack_data.mean(),\n",
    "                    'Benign_Std': benign_data.std(),\n",
    "                    'Attack_Std': attack_data.std(),\n",
    "                    'Mean_Ratio': attack_data.mean() / benign_data.mean() if benign_data.mean() != 0 else np.inf\n",
    "                })\n",
    "\n",
    "if comparison_stats:\n",
    "    comp_df = pd.DataFrame(comparison_stats)\n",
    "    comp_df = comp_df.round(4)\n",
    "    display(comp_df)\n",
    "else:\n",
    "    print(\"No valid data for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b5ca8",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624751cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "\n",
    "# Calculate quality metrics\n",
    "total_rows = len(df_sample)\n",
    "total_cols = len(df_sample.columns)\n",
    "\n",
    "# Missing values\n",
    "total_missing = df_sample.isnull().sum().sum()\n",
    "missing_percentage = (total_missing / (total_rows * total_cols)) * 100\n",
    "\n",
    "# Duplicate rows\n",
    "duplicate_rows = df_sample.duplicated().sum()\n",
    "duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "\n",
    "# Infinite values in numerical columns\n",
    "total_inf = 0\n",
    "for col in numerical_cols:\n",
    "    total_inf += np.isinf(df_sample[col]).sum()\n",
    "\n",
    "# Constant columns (zero variance)\n",
    "constant_cols = []\n",
    "for col in numerical_cols:\n",
    "    if df_sample[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "# Quality summary\n",
    "quality_report = {\n",
    "    'Total Records': f\"{total_rows:,}\",\n",
    "    'Total Features': f\"{total_cols:,}\",\n",
    "    'Missing Values': f\"{total_missing:,} ({missing_percentage:.2f}%)\",\n",
    "    'Duplicate Rows': f\"{duplicate_rows:,} ({duplicate_percentage:.2f}%)\",\n",
    "    'Infinite Values': f\"{total_inf:,}\",\n",
    "    'Constant Features': f\"{len(constant_cols)}\",\n",
    "    'Numerical Features': f\"{len(numerical_cols)}\",\n",
    "    'Categorical Features': f\"{len(df_sample.select_dtypes(include=['object']).columns)}\"\n",
    "}\n",
    "\n",
    "for key, value in quality_report.items():\n",
    "    print(f\"{key:<20}: {value}\")\n",
    "\n",
    "# Data quality score\n",
    "quality_score = 100\n",
    "if missing_percentage > 5:\n",
    "    quality_score -= min(missing_percentage * 2, 30)\n",
    "if duplicate_percentage > 1:\n",
    "    quality_score -= min(duplicate_percentage, 20)\n",
    "if total_inf > 0:\n",
    "    quality_score -= 10\n",
    "if len(constant_cols) > 0:\n",
    "    quality_score -= len(constant_cols)\n",
    "\n",
    "quality_score = max(quality_score, 0)\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Data Quality Score: {quality_score:.1f}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    print(\"âœ… Excellent data quality!\")\n",
    "elif quality_score >= 70:\n",
    "    print(\"âœ¨ Good data quality with minor issues\")\n",
    "elif quality_score >= 50:\n",
    "    print(\"âš ï¸ Moderate data quality - needs attention\")\n",
    "else:\n",
    "    print(\"âŒ Poor data quality - significant preprocessing required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca429283",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Preprocessing Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ce0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PREPROCESSING RECOMMENDATIONS ===\")\n",
    "recommendations = []\n",
    "\n",
    "# Missing values\n",
    "if total_missing > 0:\n",
    "    recommendations.append(\"ðŸ”§ Handle missing values using imputation or removal\")\n",
    "\n",
    "# Infinite values\n",
    "if total_inf > 0:\n",
    "    recommendations.append(\"ðŸ”§ Replace infinite values with NaN or large finite numbers\")\n",
    "\n",
    "# Duplicates\n",
    "if duplicate_rows > 0:\n",
    "    recommendations.append(\"ðŸ”§ Remove duplicate rows to avoid data leakage\")\n",
    "\n",
    "# Constant features\n",
    "if constant_cols:\n",
    "    recommendations.append(f\"ðŸ”§ Remove {len(constant_cols)} constant features (zero variance)\")\n",
    "\n",
    "# High correlations\n",
    "if high_corr_pairs:\n",
    "    recommendations.append(f\"ðŸ”§ Consider removing {len(high_corr_pairs)} highly correlated feature pairs\")\n",
    "\n",
    "# Class imbalance\n",
    "if benign_ratio > 90 or benign_ratio < 10:\n",
    "    recommendations.append(\"ðŸ”§ Address severe class imbalance using SMOTE, undersampling, or weighted models\")\n",
    "elif benign_ratio > 80 or benign_ratio < 20:\n",
    "    recommendations.append(\"ðŸ”§ Consider techniques for moderate class imbalance\")\n",
    "\n",
    "# Feature scaling\n",
    "recommendations.append(\"ðŸ”§ Apply feature scaling (StandardScaler or MinMaxScaler)\")\n",
    "\n",
    "# Feature selection\n",
    "recommendations.append(\"ðŸ”§ Perform feature selection to reduce dimensionality\")\n",
    "\n",
    "# Time series\n",
    "recommendations.append(\"ðŸ”§ Create time-based windows for time series analysis\")\n",
    "\n",
    "# Display recommendations\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Ready to proceed with Big Data preprocessing using Apache Spark!\")\n",
    "print(f\"ðŸ’¾ Next step: Run the Spark preprocessing pipeline to handle the full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a1038",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Run Spark Preprocessing**: Execute `spark/merge_clean.py` to process all CSV files\n",
    "2. **Feature Engineering**: Run `spark/feature_engineering.py` for advanced features\n",
    "3. **Time Series Analysis**: Create temporal windows and sequence features\n",
    "4. **Model Building**: Train Random Forest, XGBoost, and LSTM models\n",
    "5. **Real-time IDS**: Implement streaming detection pipeline\n",
    "\n",
    "This EDA provides the foundation for understanding the CSE-CIC-IDS2018 dataset structure and quality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
